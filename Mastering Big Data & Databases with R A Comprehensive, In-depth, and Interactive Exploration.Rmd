---
title: "Mastering Databases and Big Data with R"
subtitle: "A Comprehensive Guide with Kenyan Perspectives"
author: "Daniel Wanjala Machimbo"
date: "Presentation Date: May 20, 2025 (Content Prepared: `r Sys.Date()`)"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    theme: cosmo # A clean, modern theme
    highlight: pygments # Syntax highlighting style
    df_print: kable # Nicely formatted data frames
    code_folding: hide #can expand code
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.align = 'center', fig.width=9, fig.height=5.5)

# Core Packages
library(DBI)
library(RSQLite) # For SQLite examples
library(dplyr)
library(dbplyr)   # For using dplyr with databases

# Data Handling & Big Data Packages
library(data.table) # For fread/fwrite and efficient data manipulation
library(readr)      # For read_csv_chunked
library(arrow)      # For Parquet and larger-than-memory datasets

# Visualization
library(ggplot2)
library(viridis) # For nice color scales

# Example Datasets
# install.packages(c("nycflights13")) # Ensure this is installed
library(nycflights13) # For database examples with flights data

# Set a seed for reproducibility in simulations
set.seed(2024)
```

# Prelude: A Word on Data in Our Exploration (Approx. 2 minutes)

Karibu! Today, we explore the powerful capabilities of R in managing and analyzing data, particularly when dealing with structured databases and large-scale datasets ("big data").

A quick note on the datasets we'll use:

1.  **Simulated Kenyan Scenarios:** To make concepts directly relatable, we will create small, illustrative datasets within our R code. These will mimic real-world Kenyan contexts (e.g., county statistics, transactional data).These are ***NOT*** official KNBS datasets but are crafted for clear demonstration.

2.  **Standard R Packages:** We'll also use datasets from established R packages like `nycflights13`. While US-centric, the principles of handling this data (e.g., millions of flight records) are directly transferable to analyzing, say, transportation logs from the Kenya National Highways Authority (KeNHA) or logistics data for a large distributor.

3.  **Focus on Principles:** The primary goal is to understand *how* R tools work, so you can apply them to *any* dataset you encounter, including authentic Kenyan data from sources like the Kenya National Bureau of Statistics (KNBS) open data portal or specific organisational databases.

Let's begin!

------------------------------------------------------------------------

# Part 1: The Art and Science of Databases in R (Approx. 25 minutes)

## 1.1 What is a Database? A Clinical Definition

A **database** is a systematically organized or structured collection of data, typically stored electronically in a computer system. It is designed for efficient storage, retrieval, management, and updating of information. Key characteristics include:

-   **Persistence:** Data endures beyond the life of a single application session.
-   **Structure:** Data is organized according to a predefined schema (e.g., tables with rows and columns, defined data types).
-   **Controlled Access:** Mechanisms for data security, integrity, and concurrency control are implemented.
-   **Data Independence:** The way data is stored (physical schema) can be changed without necessarily affecting how applications interact with it (logical schema).
-   **Query Language:** Typically provides a specialized language (most commonly SQL - Structured Query Language) for data manipulation and retrieval.

Think of it as a highly organized, secure, and efficient digital filing cabinet, purpose-built for information management.

***Example:*** *Imagine the* **Social Health Authority** *(SHA) system. It's a massive database holding records for millions of Kenyans: personal details, contribution history, claims made, accredited hospitals. This isn't just a list; it's a structured system ensuring data accuracy (e.g., a member ID links to one person), security (protecting sensitive health info), and allowing multiple operations simultaneously (new registrations, claim processing, report generation).*

## 1.2 The `DBI` Package: R's Universal Translator for Databases

The `DBI` (Database Interface) package is the cornerstone of database interaction in R. It offers a **standardized API** (Application Programming Interface) – a common set of functions – to communicate with a wide variety of database systems.

**The `DBI` Philosophy:** Learn it once, apply it to many.

1\. `dbConnect()`: Establish a connection using a specific database driver.

2\. `dbWriteTable()`: Transfer an R data frame into a database table.

3\. `dbListTables()`: See what tables exist in the database.

4.  `dbGetQuery()`: Send an SQL query and retrieve all results as an R data frame.

5\. `dbSendQuery()`, `dbFetch()`: For more granular control, especially with large results (send query, then fetch results in pieces).

6\. `dbExecute()`: Execute SQL commands that don't return data (e.g., `CREATE TABLE`, `UPDATE`, `DELETE`).

7\. `dbDisconnect()`: Gracefully close the database connection.

### Example: `RSQLite` with Simulated Kenyan County Data

We'll use `RSQLite` which embeds a SQLite database (a lightweight, file-based database) directly into R. Let's simulate some basic data for Kenyan counties.

```{r dbi_sqlite_kenya_data_setup}
# Simulate some Kenyan county data
kenyan_county_data <- data.frame(
  County_Code = 1:5, # First 5 counties for brevity
  County_Name = c("Mombasa", "Kwale", "Kilifi", "Tana River", "Lamu"),
  Population_2019 = c(1208333, 866820, 1453787, 315943, 143920),
  Key_Industry = c("Tourism & Logistics", "Mining & Agriculture", "Tourism & Fishing", "Pastoralism & Agriculture", "Fishing & Tourism")
)

# Connect to an in-memory SQLite database.
# For a persistent database, use a file path: e.g., "kenya_data.sqlite"
con_kenya <- dbConnect(RSQLite::SQLite(), ":memory:") # In-memory for this demo

# Write our simulated data to a table named 'CountyDemographics'
dbWriteTable(con_kenya, "CountyDemographics", kenyan_county_data, overwrite = TRUE) # overwrite = TRUE for demo repeatability

cat("Simulated Kenyan County Data (first 3 rows):\n")
print(head(kenyan_county_data, 3))
```

-   **Interpretation of Simulated Data:**

-   The `kenyan_county_data` data frame we created is a small, tabular representation.

-   `County_Code`: A unique numerical identifier for each county.

-   `County_Name`: The official name of the county.

-   `Population_2019`: The population figure, hypothetically from a census.

-   `Key_Industry`: A simplified primary economic activity. This data is now ready to be written to our SQLite database.

### Interacting with the Database

```{r dbi_sqlite_kenya_interact}
# List tables in our database
cat("Tables in the 'con_kenya' database:\n")
db_tables <- dbListTables(con_kenya)
print(db_tables)

# Retrieve all data from 'CountyDemographics'
all_county_data_from_db <- dbGetQuery(con_kenya, "SELECT * FROM CountyDemographics")
cat("\nAll data retrieved from 'CountyDemographics' table:\n")
print(all_county_data_from_db)

# Retrieve counties with population greater than 1,000,000
high_pop_counties <- dbGetQuery(con_kenya, "SELECT County_Name, Population_2019 FROM CountyDemographics WHERE Population_2019 > 1000000")
cat("\nCounties with population > 1,000,000:\n")
print(high_pop_counties)

# Don't forget to disconnect!
dbDisconnect(con_kenya)
cat("\nDisconnected from the SQLite database 'con_kenya'.\n")
```

**Interpretation of Database Interaction Output:**

-   `dbListTables(con_kenya)` shows us that our table, `CountyDemographics`, was successfully created within the SQLite database.

-   The first `dbGetQuery` call (`SELECT * FROM CountyDemographics`) fetches all columns and rows from our table, mirroring the original `kenyan_county_data` we inserted. This confirms the data is stored correctly.

-   The second `dbGetQuery` demonstrates a basic SQL `WHERE` clause. It filters and returns only the names and populations of counties (`Mombasa`, `Kilifi`) whose 2019 population exceeds 1 million. This is a fundamental database operation: querying for specific subsets of data.

## 1.3 `dplyr` & `dbplyr`: Speaking R to Your Database

The `dbplyr` package is a "creative genius" itself! It allows you to write familiar `dplyr` code (verbs like `filter`, `select`, `mutate`, `group_by`, `summarise`) that `dbplyr` translates into SQL queries executed directly by the database.

**Advantages:**

-   **Familiar Syntax:** Reduces the cognitive load of switching between R and SQL.

-   **Database-Side Processing:** Computations happen in the database, which is optimized for them. Only the results are brought into R. This is crucial for large datasets.

-   **Lazy Evaluation:** `dplyr` commands on database tables build up a query plan. The SQL is executed only when you explicitly request data (e.g., via `collect()`) or compute a result.

### Example: Analyzing `nycflights13` Data (Analogous to Kenyan Aviation/Logistics)

Let's use the `flights` data from `nycflights13`. Imagine this dataset represented all flights handled by the Kenya Airports Authority (KAA) or logistics movements for a large East African company.

```{r dbplyr_flights_setup}
# Establish a new in-memory SQLite connection
con_flights_dplyr <- dbConnect(RSQLite::SQLite(), ":memory:")

# Copy the 'flights' and 'airports' data from nycflights13 into our SQLite database
# In a real-world scenario, this data would already reside in an enterprise database.
copy_to(con_flights_dplyr, nycflights13::flights, "flights_log", temporary = FALSE, overwrite = TRUE,
        indexes = list(c("year", "month", "day"), "carrier", "origin", "dest"))
copy_to(con_flights_dplyr, nycflights13::airports, "airport_details", temporary = FALSE, overwrite = TRUE,
        indexes = list("faa"))


cat("Tables in 'con_flights_dplyr' database:\n")
print(dbListTables(con_flights_dplyr))

# Create 'table references' (these don't pull data into R yet)
flights_db_ref <- tbl(con_flights_dplyr, "flights_log")
airports_db_ref <- tbl(con_flights_dplyr, "airport_details")

cat("\nStructure of the 'flights_db_ref' (a remote table reference):\n")
print(flights_db_ref) # Notice it shows source and column names, not the data itself.
```

**Interpretation of `dbplyr` Setup:** \* `copy_to(...)` has efficiently moved the `nycflights13::flights` and `nycflights13::airports` R data frames into two tables, `flights_log` and `airport_details` respectively, within our SQLite database `con_flights_dplyr`. Indexes are suggested to potentially speed up queries on those columns. \* `tbl(con_flights_dplyr, "flights_log")` creates `flights_db_ref`. This object is not the data itself, but a *pointer* or *reference* to the `flights_log` table in the database. The output confirms this by showing `Source: table<flights_log> [?? x 19]`. The `??` indicates that `dbplyr` doesn't know the exact row count without querying, reinforcing that data isn't in R's memory yet.

### Querying with `dplyr` Verbs

```{r dbplyr_flights_query}
# Find the average departure delay for flights from JFK to Los Angeles (LAX)
# broken down by carrier.
jfk_to_lax_delays <- flights_db_ref %>%
  filter(origin == "JFK", dest == "LAX", !is.na(dep_delay)) %>%
  group_by(carrier) %>%
  summarise(
    average_departure_delay = mean(dep_delay, na.rm = TRUE),
    number_of_flights = n() # dplyr::n() translates to SQL COUNT(*)
  ) %>%
  filter(number_of_flights > 10) %>% # Only carriers with a decent number of flights
  arrange(desc(average_departure_delay))

# Show the SQL query that dbplyr generated
cat("SQL query generated by dbplyr:\n")
show_query(jfk_to_lax_delays)

# Execute the query and bring results into R
jfk_to_lax_delays_local <- jfk_to_lax_delays %>% collect()

cat("\nAverage departure delay (JFK to LAX) by carrier (collected into R):\n")
print(jfk_to_lax_delays_local)

# Let's join with airport data to get airport names
# Suppose we want to see top 5 destination airports from "EWR" by flight count
top_ewr_destinations <- flights_db_ref %>%
  filter(origin == "EWR") %>%
  group_by(dest) %>%
  summarise(flight_count = n()) %>%
  arrange(desc(flight_count)) %>%
  head(5) %>% # Take top 5 in the database query itself
  left_join(airports_db_ref, by = c("dest" = "faa")) %>% # Join to get airport name
  select(destination_airport_name = name, flight_count) %>%
  collect()

cat("\nTop 5 destinations from EWR with airport names:\n")
print(top_ewr_destinations)


dbDisconnect(con_flights_dplyr)
cat("\nDisconnected from 'con_flights_dplyr' database.\n")
```

**Interpretation of `dbplyr` Querying:**

-   `show_query(jfk_to_lax_delays)` reveals the SQL that `dbplyr` constructed from our `dplyr` chain. Notice how `filter` translates to `WHERE`, `group_by` to `GROUP BY`, `summarise` with `mean()` and `n()` to `AVG()` and `COUNT()`, and `arrange` to `ORDER BY`. This is the magic: R code generating database code.

-   `jfk_to_lax_delays_local`: The `collect()` call executes the SQL and brings the aggregated results into R. The table shows carriers flying from JFK to LAX, their average departure delay for this route, and the count of such flights. For example, Endeavor Air Inc. ("9E") had, on average, the highest departure delay among carriers with more than 10 flights on this route.

-   `top_ewr_destinations`: This demonstrates a join operation. The `dplyr` code first found the top 5 destination airport codes (`dest`) from "EWR", then joined this result with the `airport_details` table (using the `faa` code as the key) to fetch the full airport names (`name`). The final collected table shows these airport names and their corresponding flight counts from EWR, ordered by popularity. This entire complex operation was planned in R, executed efficiently by the database.

------------------------------------------------------------------------

# Part 2: Conquering "Big Data" with R (Approx. 30 minutes)

## 2.1 What is "Big Data"? A Clinical Definition in the R Context

While "Big Data" is broadly defined by the "Vs" (Volume, Velocity, Variety, Veracity, Value), from the perspective of an R user on a typical machine, **Big Data pragmatically refers to any dataset that poses significant challenges to being processed effectively within the available system memory (RAM) or within acceptable timeframes using standard R functions.**

This often manifests as:

**Volume:** The dataset is too large in gigabytes (GB) or terabytes (TB) to be loaded entirely into R's memory using functions like `read.csv()`. This leads to "cannot allocate vector of size..." errors.

**Computational Intensity:** Even if a dataset fits in memory, operations on it (e.g., joins, aggregations, complex calculations on millions/billions of rows) are excessively slow with base R or non-optimized approaches.

**Velocity/Variety (Secondary in R context):** While R can handle streaming data (`pins`, `kafkaR`) or diverse data types (text, images), the primary "big data" bottleneck for many R users is often memory constraints with tabular data.

***Relatable Examle:*** *Imagine analyzing the complete M-Pesa transaction log for a single day across Kenya. This could be millions, if not tens of millions, of records. Loading this entire dataset into R on a standard laptop using `read.csv()` would likely fail due to insufficient RAM. This is a "Volume" challenge.* *Alternatively, consider detailed GPS tracking data from thousands of matatus in Nairobi over a year, recorded every few seconds. The sheer number of data points presents a significant analytical hurdle.*

## 2.2 Strategies for Taming Large Datasets in R

### Strategy 1: Efficient Data Ingestion – `data.table::fread()`

Base R's `read.csv()` is not optimized for speed or memory on large files. `data.table::fread()` is a superhero here.

```{r fread_demo_prep}
# Simulate a larger CSV file (e.g., representing customer transaction data)
# For demonstration, 2 million rows.
if (!file.exists("kenya_sim_transactions.csv")) {
  cat("Creating 'kenya_sim_transactions.csv' (2 million rows). This may take a moment...\n")
  num_transactions <- 2e6
  sim_transactions <- data.frame(
    transaction_id = paste0("TRX", 1:num_transactions),
    customer_id = sample(paste0("CUST", 1000:5000), num_transactions, replace = TRUE),
    transaction_amount_ksh = round(rlnorm(num_transactions, meanlog = log(1500), sdlog = log(2.5)), 2), # Log-normal distribution for amounts
    transaction_date = as.Date("2023-01-01") + sample(0:364, num_transactions, replace = TRUE),
    branch_code = sample(paste0("BRANCH", c("001", "002", "003", "004", "005")), num_transactions, replace = TRUE)
  )
  data.table::fwrite(sim_transactions, "kenya_sim_transactions.csv")
  rm(sim_transactions) # Remove from memory
  gc()
  cat("'kenya_sim_transactions.csv' created.\n")
} else {
  cat("'kenya_sim_transactions.csv' already exists.\n")
}
```

**Interpretation of Simulated Transaction Data:** We've created

`kenya_sim_transactions.csv` with 2 million rows, simulating a dataset of financial transactions.

`transaction_id`: A unique ID for each transaction.

`customer_id`: Identifier for the customer making the transaction.

`transaction_amount_ksh`: The value of the transaction in Kenyan Shillings, simulated to follow a log-normal distribution (common for monetary values).

`transaction_date`: The date of the transaction.

`branch_code`: Code for the branch where the transaction occurred. This file size, while not enormous, is large enough to show performance differences in reading.

```{r fread_demo_run}
# Reading with data.table::fread()
cat("Timing data.table::fread() to read 'kenya_sim_transactions.csv':\n")
time_fread <- system.time({
  dt_transactions <- data.table::fread("kenya_sim_transactions.csv", nThread = max(1, RcppParallel::defaultNumThreads() -1))
})
print(time_fread)
cat("\nDimensions of data read by fread:", dim(dt_transactions), "\n")
cat("Object size:", format(object.size(dt_transactions), units = "auto"), "\n")
cat("First 3 rows of transaction data:\n")
print(head(dt_transactions, 3))

# For comparison, you could try base R's read.csv (it would be slower)
time_read_csv <- system.time({
   df_transactions_base <- read.csv("kenya_sim_transactions.csv")
 })
 print(time_read_csv) # This would show fread is much faster
 rm(df_transactions_base) # Clean up if you run it

rm(dt_transactions) # Clean up memory for next steps
gc()
```

**Interpretation of `fread` Performance:**

-   The `system.time()` output shows how long `fread` took to read the 2 million row CSV. The "elapsed" time is the key metric. `fread` is known for being significantly faster and more memory-efficient than base R's `read.csv()`.

-   `dim(dt_transactions)` confirms that all 2 million rows and 5 columns were loaded.

-   `object.size()` shows the memory footprint of the loaded data table in R.

-   `head()` displays the first few rows, verifying the data was read correctly. The output matches our simulated structure.

### Strategy 2: Processing Data in Chunks – `readr::read_csv_chunked()`

When data is too vast to fit into memory even with `fread`, process it in manageable chunks.

```{r read_csv_chunked_kenya_demo}
# Callback function to process each chunk:
# For example, calculate total transaction amount per branch from each chunk.
results_list <- list() # To store results from each chunk

chunk_processor_kenya <- function(chunk, pos) {
  # cat("Processing chunk starting at position:", pos, "with", nrow(chunk), "rows\n") # For debugging
  processed_chunk_summary <- as.data.table(chunk)[, .(Total_Amount_Ksh = sum(transaction_amount_ksh, na.rm = TRUE)), by = branch_code]
  results_list <<- append(results_list, list(processed_chunk_summary))
}

cat("Processing 'kenya_sim_transactions.csv' in chunks of 500,000 rows:\n")
read_csv_chunked(
  "kenya_sim_transactions.csv",
  callback = DataFrameCallback$new(chunk_processor_kenya),
  chunk_size = 500000, # Process 500k rows at a time
  progress = TRUE
)

# Combine results from all chunks
if (length(results_list) > 0) {
  final_branch_summary <- rbindlist(results_list)[, .(Grand_Total_Amount_Ksh = sum(Total_Amount_Ksh)), by = branch_code]
  cat("\nAggregated transaction summary by branch (from all chunks):\n")
  print(final_branch_summary[order(branch_code)])
} else {
  cat("\nNo results collected from chunks.\n")
}
rm(results_list, final_branch_summary)
gc()
```

**Interpretation of Chunked Processing:**

-   The `read_csv_chunked` function processes `kenya_sim_transactions.csv` not all at once, but in segments of 500,000 rows.

-   The `chunk_processor_kenya` function is applied to each segment. Inside this function, we convert the chunk to a `data.table` and calculate the sum of `transaction_amount_ksh` for each `branch_code` *within that chunk*.

-   The `results_list` accumulates these per-chunk summaries.

-   After all chunks are processed, `rbindlist(results_list)` combines these intermediate summaries. The subsequent `data.table` operation `[, .(Grand_Total_Amount_Ksh = sum(Total_Amount_Ksh)), by = branch_code]` then calculates the final total transaction amount for each branch across *all* chunks.

-   The output table shows the `branch_code` and the `Grand_Total_Amount_Ksh` derived by this chunked approach. This demonstrates how to compute aggregates on a file too large to load entirely, by processing it piece by piece.

    ### Strategy 3: The `data.table` Package – Power and Speed In-Memory

If data *can* fit in RAM (perhaps after `fread`), `data.table` provides a very fast and memory-efficient syntax for manipulation. `DT[i, j, by]`

`i`: Row filtering (like `WHERE` in SQL) \* `j`: Column selection/computation (like `SELECT` in SQL) `by`: Grouping (like `GROUP BY` in SQL)

```{r datatable_ops_kenya_demo}
# Re-read the transaction data for data.table operations
dt_transactions_ops <- data.table::fread("kenya_sim_transactions.csv", nThread = max(1, RcppParallel::defaultNumThreads() -1))
cat("Transaction data loaded into data.table, dimensions:", dim(dt_transactions_ops), "\n")

# Example 1: Find average transaction amount and count for transactions in January 2023
cat("\nAggregating January 2023 transactions with data.table:\n")
# Convert transaction_date to Date type if it's not already (fread usually does this well)
if(!inherits(dt_transactions_ops$transaction_date, "Date")) {
  dt_transactions_ops[, transaction_date := as.IDate(transaction_date)] # Use IDate for efficiency
}

jan_summary_dt <- dt_transactions_ops[
  format(transaction_date, "%Y-%m") == "2023-01", # i: filter for January 2023
  .(avg_amount_ksh = mean(transaction_amount_ksh), num_transactions = .N), # j: compute mean and count
  by = branch_code # by: group by branch
][order(branch_code)] # Chain an order operation

print(jan_summary_dt)

# Example 2: Add a column: transaction fee (e.g., 0.5% of amount, capped at 50 Ksh)
# This modifies dt_transactions_ops by reference (efficient!)
dt_transactions_ops[, transaction_fee_ksh := pmin(transaction_amount_ksh * 0.005, 50)]

cat("\nFirst few transactions with the new 'transaction_fee_ksh' column:\n")
print(dt_transactions_ops[1:5, .(transaction_id, transaction_amount_ksh, transaction_fee_ksh)])

rm(dt_transactions_ops, jan_summary_dt)
gc()
```

**Interpretation of `data.table` Operations:**

-   `jan_summary_dt`: This `data.table` operation first filters rows where `transaction_date` falls in "2023-01" (`i` argument). Then, for these filtered rows, it calculates the average transaction amount (`avg_amount_ksh`) and the count of transactions (`num_transactions`), grouping the results by `branch_code` (`by` argument). The output table shows these statistics for each branch for January 2023. For instance, BRANCH001 had `num_transactions` transactions in January 2023 with an `avg_amount_ksh`.

-   The second operation demonstrates adding a new column `transaction_fee_ksh` *in-place*. The fee is calculated as 0.5% of the `transaction_amount_ksh`, but capped at a maximum of 50 Ksh (using `pmin`). The `:=` operator in `data.table` is key for this efficient, by-reference modification. The printed head of the table shows this new fee calculated for the first few transactions.

### Strategy 4: `arrow` Package – Beyond-Memory Analytics & Efficient File Formats

The Apache Arrow project (and its R package `arrow`) is a game-changer for:

-   **Columnar Memory Format:** Efficient for analytics.

-   **Zero-Copy Reads:** Fast data access between systems (e.g., R and Python).

-   **Parquet File Format:** Highly compressed, columnar storage ideal for big data.

-   **Larger-than-Memory Datasets:** `arrow` can query datasets stored on disk (e.g., in Parquet format) using `dplyr` syntax, without loading everything into R's RAM.

```{r arrow_kenya_demo_setup}
# Using the flights data for a larger example
# Imagine this is a massive log of Safaricom data bundles usage across counties, partitioned by month.
data(flights, package = "nycflights13") # Load ~336k rows

# Path for our Arrow dataset (will be a directory)
kenya_sim_usage_arrow_path <- "kenya_sim_usage_pq"

# Clean up if it exists from a previous run
if (dir.exists(kenya_sim_usage_arrow_path)) {
  unlink(kenya_sim_usage_arrow_path, recursive = TRUE)
}

cat("Writing simulated usage data (using flights data) to Parquet, partitioned by month...\n")
# For this simulation, let's rename some columns to fit a "usage" scenario
flights_renamed <- flights %>%
  select(
    user_id = flight, # Treat flight number as a user ID
    county_code = origin, # Origin airport as a county code
    usage_mb = distance, # Distance as data usage in MB
    year, month, day, hour
  )

write_dataset(
  dataset = flights_renamed,
  path = kenya_sim_usage_arrow_path,
  format = "parquet",
  partitioning = c("year", "month") # Partition data into folders by year/month
)
cat("Simulated usage data written as partitioned Parquet dataset to:", kenya_sim_usage_arrow_path, "\n")
```

**Interpretation of `arrow` Setup:**

-   We're repurposing the `nycflights13::flights` data to simulate a "Kenyan data usage" scenario. Columns are renamed: `flight` becomes `user_id`, `origin` becomes `county_code`, and `distance` becomes `usage_mb`. This helps in contextualizing.

-   `write_dataset(...)` is the key `arrow` function here. It takes the `flights_renamed` data frame and writes it to disk in the **Parquet format**. Parquet is columnar, highly efficient for storage and analytical queries. \* `partitioning = c("year", "month")`: This is crucial. `arrow` will create a directory structure within `kenya_sim_usage_pq` like `year=2013/month=1/part-0.parquet`, `year=2013/month=2/part-0.parquet`, etc. This partitioning allows `arrow` to read only the necessary data files when queries involve filters on `year` or `month`, dramatically speeding up queries on very large datasets.

```{r arrow_kenya_demo_query}
# Open the Parquet dataset (this does NOT load all data into RAM)
sim_usage_ds <- open_dataset(kenya_sim_usage_arrow_path, format = "parquet")

cat("\nSchema of the Arrow dataset (on disk):\n")
print(sim_usage_ds) # Shows schema and partitioning keys

# Query with dplyr: Total usage in MB for "county" JFK in Jan & Feb 2013
# This query is processed by Arrow's engine, often without pulling all data into R
total_usage_jfk_janfeb <- sim_usage_ds %>%
  filter(county_code == "JFK" & month %in% c(1, 2)) %>% # Filters applied efficiently due to partitioning
  summarise(total_usage_for_jfk_jan_feb_mb = sum(usage_mb, na.rm = TRUE)) %>%
  collect() # Collect the final single aggregated value

cat("\nTotal simulated data usage (MB) for 'County JFK' in Jan/Feb 2013:\n")
print(total_usage_jfk_janfeb)

# Example: Top 3 counties by average usage per user in March 2013
avg_usage_march <- sim_usage_ds %>%
  filter(month == 3) %>%
  group_by(county_code) %>%
  summarise(
    avg_mb_per_user = mean(usage_mb, na.rm = TRUE),
    distinct_users = n_distinct(user_id) # Arrow translates n_distinct
  ) %>%
  filter(distinct_users > 100) %>% # Only counties with enough users
  arrange(desc(avg_mb_per_user)) %>%
  head(3) %>% # Get top 3 in the Arrow query itself
  collect()

cat("\nTop 3 'Counties' by average usage (MB) per user in March 2013 (simulated):\n")
print(avg_usage_march)

# Clean up the Arrow dataset directory
if (dir.exists(kenya_sim_usage_arrow_path)) {
  unlink(kenya_sim_usage_arrow_path, recursive = TRUE)
}
rm(sim_usage_ds, total_usage_jfk_janfeb, avg_usage_march, flights_renamed)
gc()
```

**Interpretation of `arrow` Querying:** \* `open_dataset()` creates `sim_usage_ds`, an `Arrow Dataset` object. This is a pointer to the data on disk; the data isn't loaded into RAM yet. The printout of `sim_usage_ds` confirms its schema and highlights the partitioning keys (`year`, `month`). \* `total_usage_jfk_janfeb`: The `dplyr` query filters for `county_code == "JFK"` and `month %in% c(1, 2)`. Because the data is partitioned by `month` (and `year`), Arrow's engine can intelligently scan only the relevant Parquet files (those for January and February). The `sum(usage_mb)` aggregation is performed by Arrow, and only the final single sum is `collect()`ed into R. \* `avg_usage_march`: This demonstrates a more complex query: filtering for March, grouping by `county_code`, calculating average usage and distinct user counts, filtering again, ordering, and taking the top 3. All these steps are translated by `arrow` into an efficient execution plan that operates on the Parquet files. The `collect()` brings only the final small result (top 3 counties) into R. This is immensely powerful for large datasets.

### Strategy 5: Visualizing Large Data – Beyond the Blur

Plotting millions of points directly leads to overplotting.

Techniques:

**Sampling:** Plot a subset.

**Aggregation/Binning:** `geom_bin2d()` (rectangles), `geom_hex()` (hexagons).

**Transparency (`alpha`):** Reveals density.

```{r viz_large_kenya_demo}
# Simulate data for visualization: e.g., locations of many small businesses in a city
set.seed(123)
num_businesses <- 75000
biz_locations <- data.frame(
  # Simulate coordinates within a hypothetical city grid (e.g., Nairobi CBD and surroundings)
  longitude = rnorm(num_businesses, mean = 36.82, sd = 0.05), # Centered around Nairobi's approx longitude
  latitude = rnorm(num_businesses, mean = -1.29, sd = 0.05),  # Centered around Nairobi's approx latitude
  biz_type = sample(c("Retail", "Food", "Service", "Other"), num_businesses, replace = TRUE, prob = c(0.4, 0.3, 0.2, 0.1))
)

cat("Simulated business locations data (first 3 rows):\n")
print(head(biz_locations, 3))

# Standard scatter plot - will be heavily overplotted
p_overplot <- ggplot(biz_locations, aes(x = longitude, y = latitude)) +
  geom_point(size = 0.3, stroke=0) + # stroke=0 removes point border for denser look
  labs(title = "Simulated Business Locations (75k points) - Overplotted",
       x = "Longitude (Simulated)", y = "Latitude (Simulated)") +
  theme_minimal()
print(p_overplot)

# Scatter plot with alpha transparency
p_alpha <- ggplot(biz_locations, aes(x = longitude, y = latitude)) +
  geom_point(size = 0.3, alpha = 0.05, stroke=0) + # Low alpha to show density
  labs(title = "Business Locations with Alpha Transparency",
       x = "Longitude (Simulated)", y = "Latitude (Simulated)") +
  theme_minimal()
print(p_alpha)

# Hexagonal binning plot
p_hex <- ggplot(biz_locations, aes(x = longitude, y = latitude)) +
  geom_hex(bins = 60) + # Adjust 'bins' for different granularity
  scale_fill_viridis_c(name = "Business\nDensity") + # Using viridis for colorblind-friendly scale
  labs(title = "Business Locations - Hexagonal Binning",
       x = "Longitude (Simulated)", y = "Latitude (Simulated)") +
  theme_minimal()
print(p_hex)

rm(biz_locations, p_overplot, p_alpha, p_hex)
gc()
```

**Interpretation of Visualization Strategies:**

-   `Simulated business locations data`: We created a dataset with 75,000 points, each having a simulated longitude, latitude, and business type, conceptually representing businesses in a city like Nairobi.

-   `p_overplot`: The first plot attempts a standard scatter plot. With 75,000 points, it becomes a dense, mostly black area where individual points and density variations are hard to discern. This is **overplotting**.

-   `p_alpha`: The second plot uses the same points but with `alpha = 0.05`. This makes each point highly transparent. Where many points overlap, the color becomes more opaque. This technique helps reveal areas of high density (darker regions) versus sparse areas (lighter regions) much better than the first plot.

-   `p_hex`: The third plot uses `geom_hex()`. Instead of plotting individual points, it divides the plotting area into hexagonal bins. The color of each hexagon represents the number of data points falling within it (the density). This is an excellent way to show the spatial distribution of a large number of points, clearly highlighting "hotspots" of business concentration. The `viridis` color scale is used for better visual perception.

------------------------------------------------------------------------

# Epilogue: Your R Journey Continues (Approx. 3 minutes)(Kunywa maji, or breath in)

## Key Insights Distilled:

**Databases in R:** They are the bedrock for reliable, scalable data storage.

`DBI` is your universal key; `dbplyr` is your fluent R-to-SQL translator.

Push computations to the database whenever possible for efficiency.

**Big Data in R:**

It's about overcoming memory and speed limitations.

**No single solution:** Employ a toolkit:

Fast I/O: `data.table::fread()`.

Chunking: `readr::read_csv_chunked()` for out-of-memory processing.

In-memory muscle: `data.table` syntax for speed.

Beyond-memory analytics & efficient formats: `arrow` with Parquet.

Smart visualization: Aggregate or use transparency.

**Think strategically:** Sample, summarize, and select data wisely.

## The "Creative Genius" in You:

Mastering these tools is one part. The true "genius" emerges when you:

1\. **Ask questions** of your data.

2\. **Combine tools creatively** to solve complex problems.

3\. **Clearly communicate** your findings, supported by robust analysis.

4\. **Apply these techniques to real-world Kenyan challenges and opportunities** – from public health and agriculture to finance and urban planning.

The data landscape is vast and ever-growing. With R as your compass and these strategies as your guide, you are well-equipped to navigate it.

Asante sana! Thank you!

------------------------------------------------------------------------

# Q&A

*(Open for questions)*

------------------------------------------------------------------------

```{r final_cleanup, include=FALSE, echo=FALSE}
# Clean up simulated files created during the presentation
if (file.exists("kenya_sim_transactions.csv")) {
  unlink("kenya_sim_transactions.csv")
}
if (dir.exists("kenya_sim_usage_pq")) {
  unlink("kenya_sim_usage_pq", recursive = TRUE)
}
```
